{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6-1. Deep learning for text and sequences",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMQVt0sluaODR0S2n3ltNfz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keywoong/CoronaMap/blob/master/6_1_Deep_learning_for_text_and_sequences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InwgsrJdQfiL"
      },
      "source": [
        "# 6.1 Word-level one-hot encoding \n",
        "import numpy as np\n",
        "\n",
        "samples = ['The cat sat on the mat', 'The dog ate my homework']\n",
        "\n",
        "token_index = {}\n",
        "# 이 곳에 samples에 포함된 단어들을 dictionary형태로 저장해줄 것이다.\n",
        "\n",
        "# samples에 포함된 모든 단어들에게 고유한 정수 값들을 부여한다. 이 과정을 통해 list of integer이 생성이 되는 것이다.\n",
        "for sample in samples:\n",
        "    for word in sample.split(): # 이 줄을 통해 단어가 token임을 알 수 있다.\n",
        "        if word not in token_index: # sample의 단어들을 보다가 token_index에 없는 첫 단어를 만나게 된다면\n",
        "            token_index[word] = len(token_index) + 1 # token_index에 넣어준다. 첫 token_index의 값은 1부터 시작한다.\n",
        "\n",
        "print('>> This is token_index')\n",
        "print(token_index)\n",
        "\n",
        "max_length = 10\n",
        "# 각각의 샘플들이 가질 수 있는 최대 단어의 개수는 10개이다. \n",
        "\n",
        "results = np.zeros(shape = (len(samples), max_length,max(token_index.values()) + 1))\n",
        "# results 이곳이 바로 one-hot encoding의 결과물이다. 여기서 max(token_index.values())는 10이므로 shape은 (2,10,11)이다.\n",
        "\n",
        "# 이 과정을 통해 results 행렬에 해당 단어에 대한 값들을 넣어준다. \n",
        "# toeken_index를 보면 1부터 시작하기 때문에 배열의 첫 부분은 비어있는 것이다. \n",
        "for i, sample in enumerate(samples):\n",
        "    for j, word in list(enumerate(sample.split())) [: max_length]:\n",
        "        index = token_index.get(word)\n",
        "        results[i,j,index] = 1.\n",
        "\n",
        "print('>>This is results of one-hot encoding')\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bauJmXwSGpIj"
      },
      "source": [
        "Helllo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55L2Zw41hVCH"
      },
      "source": [
        "# 6.2 Character-level one-hot encoding \n",
        "import string\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "characters = string.printable\n",
        "# printable은 ASCII 문자열의 조합이다.\n",
        "print('>> This is characters')\n",
        "print(characters)\n",
        "# character를 출력하면 다음과 같이 아스키코드들이 출력된다.\n",
        "\n",
        "token_index = dict(zip(range(1, len(characters) + 1), characters))\n",
        "# 위에서 characters를 dictionary형태로 만들어 token_index에 넣어준다. 이는 sample들을 벡터화시킬 때 문자를 이용하는 것이 아닌 그 문자에 할당된 고유한 정수값을 이용하기 위함이다.\n",
        "# 이렇게 하여 총 100개의 아스키코드 문자들이 dictionary형태로 저장된다.\n",
        "# zip()은 동일한 개수로 이루어진 자료형을 묶어주는 역할을 하는 함수이다.\n",
        "\n",
        "print('>> This is token_index')\n",
        "print(token_index)\n",
        "\n",
        "max_length = 50\n",
        "# 한 sample당 50개 이상의 단어를 가질 수 없다.\n",
        "\n",
        "results = np.zeros((len(samples), max_length, max(token_index.keys()) + 1))\n",
        "# results는 shape이 (2, 50, 101)이다.\n",
        "\n",
        "# 이 과정을 통해 results 행렬에 해당 문자에 대한 값들을 넣어준다. \n",
        "for i, sample in enumerate(samples):\n",
        "    for j, character in enumerate(sample):\n",
        "        index = token_index.get(characters)\n",
        "        results[i,j,index] = 1.\n",
        "\n",
        "# 이렇게 하여 results를 출력하면 \n",
        "print('>> This is results of one-hot encoding')\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH1ZaxzAlKFW"
      },
      "source": [
        "# 6.3 Using keras for word-level one-hot encoding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 10)\n",
        "# sample들이 사용할 수 있는 최대 단어의 개수는 10개이다.\n",
        "\n",
        "tokenizer.fit_on_texts(samples)\n",
        "# 각각의 sample들에 포함된 단어들을 토큰화시킨다.\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(samples)\n",
        "# 토큰화시킨 단어들의 정수값들을 바탕으로 sample들을 정수값으로 변환한다.\n",
        "\n",
        "print('>> This is sequences, 정수값들로 변환시킨 sample들이다.')\n",
        "print(sequences)\n",
        "\n",
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode = 'binary')\n",
        "# 정수값으로 변환시킨 샘플들을 one-hot encoding시킨다. \n",
        "# 벡터의 크기는 10이고, 이 samples의 단어의 총 개수는 9개이므로, 1부터 9까지 각 sample이 갖고 있는 단어에 1을 넣는다.\n",
        "\n",
        "print('>>This is one_hot_results')\n",
        "print(one_hot_results)\n",
        "print('Shape is ', one_hot_results.shape)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "print('Found %s unique tokens. ' %len(word_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XgO44gqvTL1"
      },
      "source": [
        "# 6.4 Word-level one-hot encoding with hashing trick\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "dimensionality = 1000\n",
        "# 차원이 클수록 해쉬충돌의 가능성은 낮아진다.\n",
        "max_length = 10\n",
        "# 한 샘플 당 10개 이상의 단어를 가질 수 없다.\n",
        "\n",
        "results = np.zeros((len(samples), max_length, dimensionality))\n",
        "\n",
        "for i, sample in enumerate(samples):\n",
        "    for j, word in list(enumerate(sample.split())) [: max_length]:\n",
        "        index = abs(hash(word)) % dimensionality\n",
        "        #hash()라는 hashing function을 이용하여 sample의 word들을 hash한 값에 매핑시키고, 그 값에 절댓값을 씌워 정수값으로 지정한다.\n",
        "        results[i,j,index] = 1.\n",
        "\n",
        "print('The shape of results is ', results.shape)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7spK2nM8NvD"
      },
      "source": [
        "# 6.5 Instatiating an Embedding layer\n",
        "\n",
        "from keras.layers import Embedding\n",
        "embedding_layer = Embedding(1000,64)\n",
        "# 여기서 1000은 사용할 수 있는 최대 토큰(단어)의 개수이고, 64는 차원이다. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cenQqjZ_C540"
      },
      "source": [
        "# 6.6 Loading the IMDB data for use with an Embedding layer\n",
        "from keras.datasets import imdb\n",
        "from keras import preprocessing\n",
        "\n",
        "max_features = 10000\n",
        "# 입력시키는 모든 문장들에 포함되는 모든 단어의 개수는 10000를 넘어서는 안된다.\n",
        "maxlen = 20\n",
        "# 각각의 문장은 20개 이하의 문장으로 구성된다.\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_features)\n",
        "\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen = maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen = maxlen)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYS2dPHCG5tz"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igBtHBPVHEVZ"
      },
      "source": [
        "# 6.7 Using an Embedding layer and classifier on the IMDB data\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Embedding\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 8, input_length = maxlen))\n",
        "# maxlen에 딱 맞추어서 input값을 입력받는다.\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['acc'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs = 10, batch_size = 32, validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCRKdPz3IDjI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}